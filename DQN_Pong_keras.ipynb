{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DQN_Pong_keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSIiLizF1je_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model network information \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "def Fully_Connected_DQN_model():\n",
        "    lr=0.0001\n",
        "    model = models.Sequential()\n",
        "    # hidden layer takes a pre-processed frame as input, and has 200 units. Simple layer architectur of 200 x1, 1x1\n",
        "    model.add(Dense(\n",
        "        units=200,\n",
        "        input_dim=80 * 80,\n",
        "        activation='relu',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotUniform()) # Glorot is a form of Xavier Initalization https://keras.io/api/layers/initializers/#glorot_uniform\n",
        "    )\n",
        "    # output layer — we use a Sigmoid here, in order to get a 0, or 1 value to represent ACTION UP\n",
        "    model.add(Dense(\n",
        "        units=1,\n",
        "        activation='sigmoid',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotUniform())\n",
        "    )\n",
        "    # compile the model using traditional Machine Learning losses and optimizers\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer =opt,\n",
        "                  metrics = ['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "Fully_Connected_DQN_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ka9D_f1jfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "#for neural network model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "#for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def downsample(image):\n",
        "    # Take only alternate pixels - basically halves the resolution of the image (which is fine for us)\n",
        "    return image[::2, ::2, :]\n",
        "\n",
        "def remove_color(image):\n",
        "    \"\"\"Convert all color (RGB is the third dimension in the image)\"\"\"\n",
        "    return image[:, :, 0]\n",
        "\n",
        "def remove_background(image):\n",
        "    image[image == 144] = 0\n",
        "    image[image == 109] = 0\n",
        "    return image\n",
        "\n",
        "def preprocess_observations(input_observation):\n",
        "    \"\"\" convert the 210x160x3 uint8 frame into a 6400 float vector \"\"\"\n",
        "    processed_observation = input_observation[35:195] # crop\n",
        "    processed_observation = downsample(processed_observation)\n",
        "    processed_observation = remove_color(processed_observation)\n",
        "    processed_observation = remove_background(processed_observation)\n",
        "    processed_observation[processed_observation != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "    # Convert from 80 x 80 matrix to 6400 x 1 matrix\n",
        "    processed_observation = processed_observation.astype(np.float).ravel()\n",
        "\n",
        "    return processed_observation\n",
        "\n",
        "def choose_action(probability):\n",
        "    random_value = np.random.uniform()\n",
        "    if random_value < probability:\n",
        "        # signifies up in openai gym\n",
        "        return 2\n",
        "    else:\n",
        "        # signifies down in openai gym\n",
        "        return 3\n",
        "\n",
        "# reward discount used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
        "def discount_rewards(r, gamma):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  r = np.array(r)\n",
        "  discounted_r = np.zeros_like(r)\n",
        "  running_add = 0\n",
        "  # we go from last reward to first one so we don't have to do exponentiations\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
        "    running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
        "    discounted_r[t] = running_add\n",
        "  discounted_r -= np.mean(discounted_r) #normalizing the result\n",
        "  discounted_r /= np.std(discounted_r) #idem\n",
        "  return discounted_r\n",
        "\n",
        "def create_model(lr):\n",
        "    model = Sequential()\n",
        "    # hidden layer takes a pre-processed frame as input, and has 200 units. Simple layer architectur of 200 x1, 1x1\n",
        "    model.add(Dense(\n",
        "        units=200,\n",
        "        input_dim=80 * 80,\n",
        "        activation='relu',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotUniform()) # Glorot is a form of Xavier Initalization https://keras.io/api/layers/initializers/#glorot_uniform\n",
        "    )\n",
        "    # output layer — we use a Sigmoid here, in order to get a 0, or 1 value to represent ACTION UP\n",
        "    model.add(Dense(\n",
        "        units=1,\n",
        "        activation='sigmoid',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotUniform())\n",
        "    )\n",
        "    # compile the model using traditional Machine Learning losses and optimizers\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer =opt,\n",
        "                  metrics = ['accuracy'])\n",
        "    return model\n",
        "def play():\n",
        "    env = gym.make(\"Pong-v0\")\n",
        "    resume = False #change it to true if you already trained the agent previously\n",
        "    train_episodes = 700\n",
        "\n",
        "    # hyperparameters\n",
        "    epoch_update_weight = 1024 # The number of training samples to work through before the model’s weights are updated\n",
        "    epochs_number = 1 # the number times that the learning algorithm will work through the entire training dataset. \n",
        "    gamma = 0.99 # discount factor for reward\n",
        "    decay_rate = 0.99 # the exploitation rate of the agent\n",
        "    learning_rate = 1e-4 # pass in create_model(lr) to set the learning_rate for the optimizer\n",
        "    input_dimensions = 80*80\n",
        "\n",
        "    # game parameters\n",
        "    episode_observations, episode_actions, episode_rewards = [], [], []\n",
        "    reward_sum = 0\n",
        "    episode_number = 0\n",
        "    running_reward = None\n",
        "\n",
        "\n",
        "    #initialize DQN model\n",
        "    input_dimensions = 80*80\n",
        "    model = create_model(learning_rate)\n",
        "    if resume:\n",
        "        path = os.path.join('pong_model_DQN_checkpoint.h5')\n",
        "        model.load_weights(path)\n",
        "    epochs_before_saving = 100 # use for saving model weight every 100 episodes\n",
        "\n",
        "    #Plotting\n",
        "    loss_buffer = []\n",
        "    reward_buffer = []\n",
        "    total_episodes_buffer = []\n",
        "\n",
        "\n",
        "    observation = env.reset() # This gets us the image\n",
        "    prev_input = None\n",
        "    # main training loop\n",
        "    #while (True):\n",
        "    while episode_number < train_episodes:\n",
        "        #env.render() # remove \"#\" if you train the agent on your local machine. The env.render is for showing the Pong agent play the game in real time\n",
        "        cur_observations = preprocess_observations(observation)\n",
        "        processed_observations = cur_observations - prev_input if prev_input is not None else np.zeros(input_dimensions)\n",
        "        prev_input = cur_observations\n",
        "\n",
        "        # forward the policy network and sample action according to the probability distribution\n",
        "        up_probability = model.predict(np.expand_dims(processed_observations, axis=1).T)\n",
        "\n",
        "        action = choose_action(up_probability)\n",
        "        # see here: http://cs231n.github.io/neural-networks-2/#losses\n",
        "        fake_label = 1 if action == 2 else 0\n",
        "\n",
        "        # log the input and label to train later\n",
        "        episode_observations.append(processed_observations)\n",
        "        episode_actions.append(fake_label)\n",
        "\n",
        "        # carry out the chosen action\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        episode_rewards.append(reward)\n",
        "        reward_sum += reward\n",
        "\n",
        "        if done:\n",
        "            total_episodes_buffer.append(episode_number)\n",
        "\n",
        "            # discount the rewards based on the actions taken\n",
        "            episode_action_reward_discounted = discount_rewards(episode_rewards, gamma)\n",
        "\n",
        "            # training - meaning that performing backprop to update weights as well as running Gradient Descent\n",
        "            # as the fit() in keras is called, it runs the gradient descent optimizer algorithm as defined earlier\n",
        "            hist = model.fit(x=np.vstack(episode_observations),\n",
        "                             y=np.vstack(episode_actions),\n",
        "                             batch_size =epoch_update_weight,\n",
        "                             verbose=1,\n",
        "                             epochs=epochs_number,\n",
        "                             sample_weight=episode_action_reward_discounted\n",
        "                             )\n",
        "            \n",
        "            loss_buffer.append(hist.history['loss'])\n",
        "            # Saving the weights used by our model\n",
        "            if episode_number % epochs_before_saving == 0:\n",
        "                if os.path.exists('pong_model_DQN_checkpoint.h5'):\n",
        "                    os.remove('pong_model_DQN_checkpoint.h5')\n",
        "                model.save_weights('pong_model_DQN_checkpoint.h5')\n",
        "\n",
        "            observation = env.reset()  # reset env\n",
        "            # simutaniously let the angent to explore the environment at .01 rate and exploit the environment at .99 rate\n",
        "            running_reward = reward_sum if running_reward is None else running_reward * decay_rate + reward_sum *(1-decay_rate)\n",
        "            reward_buffer.append(running_reward)\n",
        "\n",
        "            print('resetting env. episode %f. episode reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "            # Reinitialization\n",
        "            episode_observations, episode_actions, episode_rewards = [], [], []\n",
        "            reward_sum = 0\n",
        "            prev_input = None\n",
        "            episode_number += 1\n",
        "    env.close() #if you run this on your local machine you need to close the env at the end\n",
        "    plt.figure(1)\n",
        "    plt.plot(total_episodes_buffer,loss_buffer)\n",
        "    plt.title('Model Loss over The number of Episodes')\n",
        "    plt.ylabel('Model Loss')\n",
        "    plt.xlabel('Episodes')\n",
        "\n",
        "    plt.figure(2)\n",
        "    plt.plot(total_episodes_buffer,reward_buffer)\n",
        "    plt.title('Rewards Earned over The number of Episodes')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.show()        \n",
        "play()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}