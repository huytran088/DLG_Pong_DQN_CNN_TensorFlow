{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN_Pong_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ4t4druzzs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#network Infor only - CNN with maxpooling\n",
        "#the agent was implement using this model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "   \n",
        "def CNN_pooling_model():   \n",
        "    inputShape= (80,80,1)\n",
        "    # CNN model: http://cscubs.cs.uni-bonn.de/2018/proceedings/paper_1.pdf\n",
        "    model = models.Sequential()\n",
        "    model.add(Conv2D(16, (3, 3), padding='same',input_shape=inputShape, data_format=\"channels_last\"))#conv1\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))#pooling\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))#conv2\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))#pooling\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3),padding='same'))#conv3\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    #fully connected \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "\n",
        "    opt = Adam(learning_rate = lr)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "#opt = Adam(learning_rate = 0.001)\n",
        "#model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "CNN_pooling_model()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RowLloxlzztD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#network Infor only - CNN WITHOUT maxpooling\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "def CNN_typical_model():\n",
        "  lr=0.0001\n",
        "  model = models.Sequential()\n",
        "#https://github.com/mlitb/pong-cnn/blob/master/model.py\n",
        "  inputShape= (80,80,1)\n",
        "  model.add(Conv2D(filters=16,kernel_size=8,strides=4,activation='relu',input_shape=inputShape,data_format=\"channels_last\"))\n",
        "  model.add(Conv2D(filters=32, kernel_size=4,strides=2,activation='relu'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256,activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  opt = Adam(learning_rate = lr)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "CNN_typical_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKqBLsKizztG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main\n",
        "import numpy as np\n",
        "import gym\n",
        "#for neural network model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "#for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def downsample(image):\n",
        "    # Take only alternate pixels - basically halves the resolution of the image (which is fine for us)\n",
        "    return image[::2, ::2, :]\n",
        "\n",
        "def remove_color(image):\n",
        "    \"\"\"Convert all color (RGB is the third dimension in the image)\"\"\"\n",
        "    return image[:, :, 0]\n",
        "\n",
        "def remove_background(image):\n",
        "    image[image == 144] = 0\n",
        "    image[image == 109] = 0\n",
        "    return image\n",
        "\n",
        "def preprocess_observations(input_observation: np.ndarray) -> np.ndarray:\n",
        "    \"\"\" Preprocess 210x160x3 uint8 frame into 1x80x80x1 4D float32 tensor.\"\"\"\n",
        "    processed_observation = input_observation[35:195] # crop\n",
        "    processed_observation = downsample(processed_observation)\n",
        "    processed_observation = remove_color(processed_observation)\n",
        "    processed_observation = remove_background(processed_observation)\n",
        "    processed_observation[processed_observation != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "    \n",
        "    processed_observation = np.expand_dims(processed_observation, 0)\n",
        "    processed_observation = np.expand_dims(processed_observation, -1)\n",
        "    return tf.convert_to_tensor(processed_observation, dtype=tf.float32)\n",
        "\n",
        "def choose_action(probability):\n",
        "    random_value = np.random.uniform()\n",
        "    if random_value < probability:\n",
        "        # signifies up in openai gym\n",
        "        return 2\n",
        "    else:\n",
        "        # signifies down in openai gym\n",
        "        return 3\n",
        "\n",
        "# reward discount used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
        "def discount_rewards(r, gamma):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  r = np.array(r)\n",
        "  discounted_r = np.zeros_like(r)\n",
        "  running_add = 0\n",
        "  # we go from last reward to first one so we don't have to do exponentiations\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
        "    running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
        "    discounted_r[t] = running_add\n",
        "  discounted_r -= np.mean(discounted_r) #normalizing the result\n",
        "  discounted_r /= np.std(discounted_r) #idem\n",
        "  return discounted_r\n",
        "\n",
        "def create_model(lr, inputShape):\n",
        "    # CNN model: http://cscubs.cs.uni-bonn.de/2018/proceedings/paper_1.pdf\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(16, (3, 3), padding='same',input_shape=inputShape, data_format=\"channels_last\"))#conv1\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))#pooling\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))#conv2\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))#pooling\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3),padding='same'))#conv3\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    #fully connected \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "\n",
        "    opt = Adam(learning_rate = lr)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "def play():\n",
        "    env = gym.make(\"Pong-v0\")\n",
        "    resume = False #change it to true if you already trained the agent previously\n",
        "    train_episodes = 700\n",
        "\n",
        "    # hyperparameters\n",
        "    epoch_update_weight = 1500 # The number of training samples to work through before the modelâ€™s weights are updated\n",
        "    epochs_number = 7 # the number of times that the learning algorithm will work through the entire training dataset. \n",
        "    gamma = 0.99 # discount factor for reward\n",
        "    decay_rate = 0.99 # the exploitation rate of the agent\n",
        "    learning_rate = 1e-4 # pass in create_model(lr) to set the learning_rate for the optimizer\n",
        "    \n",
        "\n",
        "    # game parameters\n",
        "    episode_observations, episode_actions, episode_rewards = [], [], []\n",
        "    reward_sum = 0\n",
        "    episode_number = 0\n",
        "    running_reward = None\n",
        "\n",
        "    #initialize DQN model\n",
        "    input_dimensions = (80,80,1)\n",
        "    model = create_model(learning_rate,input_dimensions)\n",
        "    if resume:\n",
        "        path = os.path.join('pong_model_checkpoint.h5')\n",
        "        model.load_weights(path)\n",
        "    epochs_before_saving = 100 # use for saving model weight every 100 episodes\n",
        "\n",
        "    #Plotting\n",
        "    loss_buffer = []\n",
        "    reward_buffer = []\n",
        "    total_episodes_buffer = []\n",
        "\n",
        "    #environment initialization\n",
        "    observation = env.reset() # This gets us the image\n",
        "    prev_input = tf.zeros((1,80, 80, 1), dtype=tf.float32) #convolutional network input is a 3D tensor size (80*80*1)\n",
        "    # main training loop\n",
        "    while episode_number < train_episodes:\n",
        "    #while True:\n",
        "        #env.render() # remove \"#\" if you train the agent on your local machine. The env.rendoer is for showing the Pong agent play the game in real time\n",
        "        cur_observations = preprocess_observations(observation)\n",
        "        processed_observations = cur_observations - prev_input if prev_input is not None else tf.zeros((1,80, 80, 1), dtype=tf.float32)\n",
        "        prev_input = cur_observations\n",
        "        # forward the policy network and sample action according to the probability distribution\n",
        "        up_probability = model.predict(processed_observations)\n",
        "\n",
        "        action = choose_action(up_probability)\n",
        "        # see here: http://cs231n.github.io/neural-networks-2/#losses\n",
        "        fake_label = 1 if action == 2 else 0\n",
        "\n",
        "        # log the input and label to train later\n",
        "        episode_observations.append(processed_observations)\n",
        "        episode_actions.append(fake_label)\n",
        "\n",
        "        # carry out the chosen action\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        episode_rewards.append(reward)\n",
        "        reward_sum += reward\n",
        "\n",
        "        if done:\n",
        "            total_episodes_buffer.append(episode_number)\n",
        "            # discount the rewards based on the actions taken\n",
        "            episode_action_reward_discounted = discount_rewards(episode_rewards, gamma)\n",
        "            # training - meaning that performing backprop to update weights as well as running Gradient Descent\n",
        "            # as the fit() in keras is called, it runs the gradient descent optimizer algorithm as defined earlier\n",
        "            hist = model.fit(x=np.vstack(episode_observations),\n",
        "                             y=np.vstack(episode_actions),\n",
        "                             batch_size=epoch_update_weight,\n",
        "                             verbose=1,\n",
        "                             epochs=epochs_number,\n",
        "                             sample_weight=episode_action_reward_discounted\n",
        "                             )\n",
        "            loss_buffer.append(hist.history['loss'])\n",
        "            # Saving the weights used by our model\n",
        "            if episode_number % epochs_before_saving == 0:\n",
        "                if os.path.exists('pong_model_checkpoint.h5'):\n",
        "                    os.remove('pong_model_checkpoint.h5')\n",
        "                model.save_weights('pong_model_checkpoint.h5')\n",
        "\n",
        "            observation = env.reset()  # reset env\n",
        "            # simutaniously let the angent to explore the environment at .01 rate and exploit the environment at .99 rate\n",
        "            running_reward = reward_sum if running_reward is None else running_reward * decay_rate + reward_sum *(1-decay_rate)\n",
        "            reward_buffer.append(running_reward)\n",
        "\n",
        "            print('resetting env. episode %f. episode reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "            # Reinitialization\n",
        "            episode_observations, episode_actions, episode_rewards = [], [], []\n",
        "            reward_sum = 0\n",
        "            prev_input = None            \n",
        "            episode_number += 1\n",
        "       \n",
        "    env.close() #if you run this on your local machine you need to close the env at the end\n",
        "    plt.figure(1)\n",
        "    plt.plot(total_episodes_buffer,loss_buffer)\n",
        "    plt.title('Model Loss over The number of Episodes')\n",
        "    plt.ylabel('Model Loss')\n",
        "    plt.xlabel('Episodes')\n",
        "\n",
        "    plt.figure(2)\n",
        "    plt.plot(total_episodes_buffer,reward_buffer)\n",
        "    plt.title('Rewards Earned over The number of Episodes')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.show()\n",
        "play()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}